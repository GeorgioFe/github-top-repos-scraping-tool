{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba45cad",
   "metadata": {},
   "source": [
    "# Top Repositories for GitHub Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93534b5",
   "metadata": {},
   "source": [
    "### Pick a website and describe your objective\n",
    "\n",
    "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342b06e",
   "metadata": {},
   "source": [
    "#### Project Outline\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics, for each topic, we'll get topic title, topic page URL, and topic description.\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page.\n",
    "- For each repository, we'll grab the repo name, username, stars, and repo URL.\n",
    "- For each topic, we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93c434",
   "metadata": {},
   "source": [
    "### Use the requests library to download web pages\n",
    "\n",
    "- Inspect the website's HTML source and identify the right URLs to download.\n",
    "- Download and save web pages locally using the requests library.\n",
    "- Create a function to automate downloading for different topics/search queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78a96c",
   "metadata": {},
   "source": [
    "### Use Beautiful Soup to parse and extract information\n",
    "\n",
    "- Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "- Use the right properties and methods to extract the required information.\n",
    "- Create functions to extract from the page into lists and dictionaries.\n",
    "- (Optional) Use a REST API to acquire additional information if required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7242f",
   "metadata": {},
   "source": [
    "### Create CSV file(s) with the extracted information\n",
    "\n",
    "- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "- Execute the function with different inputs to create a dataset of CSV files.\n",
    "- Verify the information in the CSV files by reading them back using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f392b2",
   "metadata": {},
   "source": [
    "### Document and share your work\n",
    "\n",
    "- Add proper headings and documentation in your Jupyter notebook.\n",
    "- (Optional) Write a blog post about your project and share it online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0feb5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3582b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6aacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b847e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6205383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    topic_parsed = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_parsed\n",
    "\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "\n",
    "def get_repo_info(h3_tag, star_tag):\n",
    "    \"\"\" Return all the required info about a repository. \"\"\"\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "\n",
    "def get_topic_repos(parsed_doc):\n",
    "    repo_tags = parsed_doc.find_all('h3', {'class': \"f3 color-fg-muted text-normal lh-condensed\"})\n",
    "    star_tags = parsed_doc.find_all('span', {'class': \"Counter js-social-count\"})\n",
    "    \n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo name' : [],\n",
    "        'stars' : [],\n",
    "        'repo URL' : []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo URL'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "\n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)\n",
    "\n",
    "    \n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    parsed_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topic_title_tags = parsed_doc.find_all('p', {'class': \"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "    topic_desc_tags = parsed_doc.find_all('p', {'class': \"f5 color-fg-muted mb-0 mt-1\"})\n",
    "    topic_link_tags = parsed_doc.find_all('a', {'class': \"no-underline flex-1 d-flex flex-column\"})\n",
    "    \n",
    "    topic_titles = [topic.text for topic in topic_title_tags]\n",
    "    topic_descs = [topic.text.strip() for topic in topic_desc_tags]\n",
    "    topic_urls = [base_url + topic['href'] for topic in topic_link_tags]\n",
    "    \n",
    "    topics_dict = {\n",
    "        'Title' : topic_titles,\n",
    "        'Description' : topic_descs,\n",
    "        'URL' : topic_urls\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dab57",
   "metadata": {},
   "source": [
    "### Write a single function to:\n",
    "1. Get the list of topics from the topics page.\n",
    "2. Get the list of top repos from the individual topic pages.\n",
    "3. For each topic, create a csv of the top repos for the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb8685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('Topics Data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['Title']))\n",
    "        scrape_topic(row['URL'], 'Topics Data/{}.csv'.format(row['Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bd8f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "The file Topics Data/3D.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Ajax\"\n",
      "The file Topics Data/Ajax.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "The file Topics Data/Algorithm.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Amp\"\n",
      "The file Topics Data/Amp.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Android\"\n",
      "The file Topics Data/Android.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Angular\"\n",
      "The file Topics Data/Angular.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Ansible\"\n",
      "The file Topics Data/Ansible.csv already exists. Skipping...\n",
      "Scraping top repositories for \"API\"\n",
      "The file Topics Data/API.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Arduino\"\n",
      "The file Topics Data/Arduino.csv already exists. Skipping...\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "The file Topics Data/ASP.NET.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Atom\"\n",
      "The file Topics Data/Atom.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "The file Topics Data/Awesome Lists.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "The file Topics Data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Azure\"\n",
      "The file Topics Data/Azure.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Babel\"\n",
      "The file Topics Data/Babel.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bash\"\n",
      "The file Topics Data/Bash.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "The file Topics Data/Bitcoin.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "The file Topics Data/Bootstrap.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bot\"\n",
      "The file Topics Data/Bot.csv already exists. Skipping...\n",
      "Scraping top repositories for \"C\"\n",
      "The file Topics Data/C.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Chrome\"\n",
      "The file Topics Data/Chrome.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "The file Topics Data/Chrome extension.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "The file Topics Data/Command line interface.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Clojure\"\n",
      "The file Topics Data/Clojure.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Code quality\"\n",
      "The file Topics Data/Code quality.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Code review\"\n",
      "The file Topics Data/Code review.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Compiler\"\n",
      "The file Topics Data/Compiler.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "The file Topics Data/Continuous integration.csv already exists. Skipping...\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "The file Topics Data/COVID-19.csv already exists. Skipping...\n",
      "Scraping top repositories for \"C++\"\n",
      "The file Topics Data/C++.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
